# WhisperX ASR API Service Configuration

# Hugging Face Token (REQUIRED for speaker diarization)
# Get your token from https://huggingface.co/settings/tokens
# Accept user agreements for ALL THREE models:
# - https://huggingface.co/pyannote/speaker-diarization-community-1
# - https://huggingface.co/pyannote/speaker-diarization-3.1
# - https://huggingface.co/pyannote/segmentation-3.0
HF_TOKEN=your_huggingface_token_here

# Device Configuration
# Options: cuda, cpu
DEVICE=cuda

# Compute Type
# Options: float16 (GPU), float32 (CPU), int8 (CPU, lower quality but faster)
COMPUTE_TYPE=float16

# Batch Size (adjust based on your GPU memory)
# Larger = faster but more memory
# Recommended: 16 for 8GB VRAM, 32 for 16GB+ VRAM
BATCH_SIZE=16

# Cache Directory (inside container)
# Used for WhisperX models and alignment models
CACHE_DIR=/.cache

# Hugging Face Cache Directory
# Used for pyannote diarization models - set to same as CACHE_DIR for unified caching
# This enables offline use after models are downloaded once
# Set automatically in docker-compose.yml to match CACHE_DIR
# HF_HOME=/.cache

# Offline Mode (OPTIONAL)
# Set to 1 to run in offline mode (no network requests to Hugging Face)
# Requires all models to be cached first - run once with internet access
# See README.md "Offline Use" section for details
# HF_HUB_OFFLINE=1

# Model Preloading (OPTIONAL)
# Preload a specific model on startup to reduce first-request latency
# Options: tiny, base, small, medium, large-v2, large-v3
# Leave empty or comment out to disable preloading
PRELOAD_MODEL=large-v3

# Maximum File Size (OPTIONAL)
# Maximum audio file size in MB to prevent out-of-memory errors
# Large files can cause VRAM exhaustion on the GPU
# Default: 1000MB (1GB)
# Adjust based on your GPU memory (lower for 8-16GB GPUs)
MAX_FILE_SIZE_MB=1000
